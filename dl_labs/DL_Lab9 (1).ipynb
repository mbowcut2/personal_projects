{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-jFCArcTC-B"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab9.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl7hEuB3J4k3"
      },
      "source": [
        "# Lab 9: Deep Reinforcement Learning\n",
        "\n",
        "## Objective\n",
        "\n",
        "- Build DQN and PPO Deep RL algorithms\n",
        "- Learn the difference between Q Learning and Policy Gradient techniques\n",
        "\n",
        "## Deliverable\n",
        "\n",
        "For this lab you will submit an IPython notebook via Learning Suite. This lab gives you a lot of code, and you should only need to modify two of the cells of this notebook, but you can modify any of the given code if you wish.\n",
        "\n",
        "## Tips\n",
        "\n",
        "Deep reinforcement learning is difficult. We provide hyperparameters, visualizations, and code for gathering experience, but require you to code up algorithms for training your networks. \n",
        "\n",
        "- Your networks should be able to demonstrate learning on cartpole within a minute of wall time.\n",
        "\n",
        "- Understand what your the starter code is doing. This will help you with the *TODO* sections. The main code block is similar for the two algorithms with some small yet important differences.\n",
        "\n",
        "- We provide hyperparameters for you to start with. Feel free to experiment with different values, but these worked for us.\n",
        "\n",
        "- **Print dtypes and shapes** throughout your code to make sure your tensors look the way you expect.\n",
        "\n",
        "- The DQN algorithm is significantly more unstable than PPO. Even with a correct implementation it may fail to learn every 1/10 times.\n",
        "\n",
        "- Unfortunately visualizing your agent acting in the environment is non-trivial in Colab. You can visualize your agent by running this code locally and uncommenting the `env.render()` line.\n",
        "\n",
        "## Grading\n",
        "\n",
        "- 35% Part 1: DQN *TODO* methods\n",
        "- 35% Part 2: PPO *TODO* methods\n",
        "- 20% Cartpole learning curves for DQN and PPO\n",
        "- 10% Tidy legible code\n",
        "\n",
        "___\n",
        "\n",
        "## Cartpole\n",
        "\n",
        "Cartpole is a simple environment to get your agent up and running. It has a continuous state space of 4 dimensions and a discrete action space of 2. The agent is given a reward of 1 for each timestep it remains standing. Your agent should be able to reach close to 200 cumulative reward for an episode after a minute or two of training. The below graphs show example results for dqn (left) and ppo (right).\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1Bpz1jOPMF1zJMW6XBJJ44sJ-RmO_q6_U)\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1M1yygXhLKDL8qfRXn7fh_K-zq7-pQRhY)\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVWokqnVab6O"
      },
      "source": [
        "# Starter Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaPOG6xt0yn"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyykNyRM1Tf3",
        "outputId": "28d2caec-f079-4362-c97e-21910918e9de"
      },
      "source": [
        "! pip3 install gym\n",
        "! pip3 install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rim8iocC1Vva"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from itertools import chain\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3MxvoVi7cdr"
      },
      "source": [
        "## Part 1: DQN\n",
        "\n",
        "Deep Q-Network (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) is a Q-learning algorithm that learns values for state-action pairs.\n",
        "\n",
        "Actions are sampled according to an $\\epsilon-greedy$ policy to help with exploration of the state space. Every time an action is sampled, the agent chooses a random action with $\\epsilon$ probability. Otherwise, the agent selects the action with the highest Q-value for a state. $\\epsilon$ decays over time according to $\\epsilon \\gets \\epsilon * epsilon\\_decay$.\n",
        "\n",
        "Tuples of state, action, reward, next_state, and terminal $(s,a,r,s',d)$ are collected during training. Every $learn\\_frequency$ steps $sample\\_size$ tuples are sampled and made into 5 tensors tensors of states, actions, rewarads, next_states, and terminals.\n",
        "\n",
        "The loss for a batch of size N is given below.\n",
        "\n",
        "$Loss=\\frac{1}{N}\\sum \\bigg(Q(s,a) - (r + \\gamma \\underset{a'\\sim A}{max} \\hat{Q}(s',a')(1-d))\\bigg)^2 $\n",
        "\n",
        "Loss is calculated and used to update the Q-Network. The target network $\\hat{Q}$ begins as a copy of the Q network but is not updated by the optimizer. Every $target\\_update$ steps, the target network is updated with the parameters of the Q-Network. This processes is a type of bootstrapping.\n",
        "\n",
        "### TODO\n",
        "\n",
        "- Implement get action method with e-greedy policy\n",
        "- Implement sample batch method\n",
        "- Implement DQN learning algorithm\n",
        "- Train DQN on cartpole\n",
        "- Display learning curves with average episodic reward per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mBUvXkT2dHy"
      },
      "source": [
        "def get_action_dqn(network, state, epsilon, epsilon_decay):\n",
        "  \"\"\"Select action according to e-greedy policy and decay epsilon\n",
        "\n",
        "    Args:\n",
        "        network (QNetwork): Q-Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "        epsilon (float): probability of choosing a random action\n",
        "        epsilon_decay (float): amount by which to decay epsilon\n",
        "\n",
        "    Returns:\n",
        "        action (int): chosen action [0, action_size)\n",
        "        epsilon (float): decayed epsilon\n",
        "  \n",
        "    # STRATEGY: start with high epsilon (p of exporation) and then decay over time toward exploitaion\n",
        "      # EXPLORATION\n",
        "      # EXPLOITATION -- using info we have and always choosing best possible path through data -- no exploration, not good at learning\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  state = torch.from_numpy(state).float().cuda()\n",
        "\n",
        "  if random.uniform(0,1) < epsilon:\n",
        "    action = random.randint(0,1)\n",
        "  \n",
        "  else:\n",
        "    q_vals = network(state)\n",
        "    action = torch.argmax(q_vals).item()\n",
        "\n",
        "  return action, epsilon*epsilon_decay\n",
        "\n",
        "\n",
        "def prepare_batch(memory, batch_size):\n",
        "  \"\"\"Randomly sample batch from memory\n",
        "     Prepare cuda tensors\n",
        "\n",
        "    Args:\n",
        "        memory (list): state, action, next_state, reward, done tuples\n",
        "        batch_size (int): amount of memory to sample into a batch\n",
        "\n",
        "    Returns:\n",
        "        state (tensor): float cuda tensor of size (batch_size x state_size()\n",
        "        action (tensor): long tensor of size (batch_size)\n",
        "        next_state (tensor): float cuda tensor of size (batch_size x state_size)\n",
        "        reward (tensor): float cuda tensor of size (batch_size)\n",
        "        done (tensor): float cuda tensor of size (batch_size)\n",
        "  \"\"\"\n",
        "  sample = random.choices(memory, k=batch_size)\n",
        "\n",
        "  s_list = []\n",
        "  a_list = []\n",
        "  n_list = []\n",
        "  r_list = []\n",
        "  d_list = []\n",
        "\n",
        "  for item in sample:\n",
        "    s, a, n, r, d = item\n",
        "    s = torch.from_numpy(s).float().cuda().unsqueeze(0)\n",
        "    n = torch.from_numpy(n).float().cuda().unsqueeze(0)\n",
        "    a = torch.Tensor([a]).long().cuda()\n",
        "    r = torch.Tensor([r]).cuda()\n",
        "    d = torch.Tensor([d]).cuda()\n",
        "\n",
        "    s_list.append(s)\n",
        "    a_list.append(a)\n",
        "    n_list.append(n)\n",
        "    r_list.append(r)\n",
        "    d_list.append(d)\n",
        "  \n",
        "  state = torch.cat(s_list, dim=0)\n",
        "  action = torch.cat(a_list)\n",
        "  next_state = torch.cat(n_list, dim=0)\n",
        "  reward = torch.cat(r_list)\n",
        "  done = torch.cat(d_list)\n",
        "\n",
        "  # print(\"Batch Size: \", batch_size)\n",
        "\n",
        "\n",
        "\n",
        "  return state, action, next_state, reward, done\n",
        "  \n",
        "  \n",
        "def learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update):\n",
        "  \"\"\"Update Q-Network according to DQN Loss function\n",
        "     Update Target Network every target_update global steps\n",
        "\n",
        "    Args:\n",
        "        batch (tuple): tuple of state, action, next_state, reward, and done tensors --> from prepare_batch()\n",
        "        optim (Adam): Q-Network optimizer\n",
        "        q_network (QNetwork): Q-Network\n",
        "        target_network (QNetwork): Target Q-Network --> just a copy that only updates every number of steps cuz its hard to learn on moving targets\n",
        "        gamma (float): discount factor\n",
        "        global_step (int): total steps taken in environment\n",
        "        target_update (int): frequency of target network update\n",
        "  \"\"\"\n",
        "  optim.zero_grad()\n",
        "\n",
        "  state, action, next_state, reward, done = batch\n",
        "\n",
        "  a_one_hot_encoding = nn.functional.one_hot(action.long(), num_classes=2)\n",
        "  q_vals = target_network(next_state)\n",
        "  future_reward, _ = torch.max(q_vals, dim=1)\n",
        "\n",
        "  target = reward + gamma*future_reward * (1-done) #cuz there's only a future reward if there's a future lolz\n",
        "  loss = nn.functional.mse_loss(q_network(state)[a_one_hot_encoding.bool()], target) # only want Q value for action, a, that we chose --> boolean tensor is treated as a mask, and returns values for True\n",
        "\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if global_step % target_update == 0:\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "  return "
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KidOnLu17FEw",
        "outputId": "b0f6baef-37d4-46c9-b803-bfbbd37a8fab"
      },
      "source": [
        "action = torch.Tensor([1,1,1,1,0,0,0,1,0,1,0,1,0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0]).long()\n",
        "print(action.size())\n",
        "action = action.unsqueeze(0)\n",
        "print(torch.cat([action, action], dim=0).size())"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28])\n",
            "torch.Size([2, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MnwxeT274YB"
      },
      "source": [
        ""
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJJrxQ2p7OIO",
        "outputId": "4aad047a-16be-474c-d836-c3ac5ab48e94"
      },
      "source": [
        "t = nn.functional.one_hot(torch.arange(0,5) % 3)\n",
        "print(t.size())"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW1kTDW_AERR",
        "outputId": "18cbe003-9460-492b-e191-7ed0d9bb241f"
      },
      "source": [
        "torch.max(t, dim=1)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([1, 1, 1, 1, 1]), indices=tensor([0, 1, 2, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGQgiY0WvImB"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vMhl-oevIBo"
      },
      "source": [
        "# Q-Value Network\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "    \n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, action_size))  \n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"Estimate q-values given state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          q-values (tensor): estimated q-values, size (batch x action_size)\n",
        "    \"\"\"\n",
        "    return self.net(x)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCafVI552dgg"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sy_r9Wr2eg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88241c6a-dd65-459a-86aa-10189ce8df76"
      },
      "source": [
        "def dqn_main():\n",
        "  # Hyper parameters\n",
        "  lr = 1e-3\n",
        "  epochs = 500 #probably increase this to ~750\n",
        "  start_training = 1000\n",
        "  gamma = 0.99\n",
        "  batch_size = 32\n",
        "  epsilon = 1\n",
        "  epsilon_decay = .9999\n",
        "  target_update = 1000\n",
        "  learn_frequency = 2\n",
        "\n",
        "  # Init environment\n",
        "  state_size = 4\n",
        "  action_size = 2\n",
        "  env = gym.make('CartPole-v1', )\n",
        "\n",
        "  # Init networks\n",
        "  q_network = QNetwork(state_size, action_size).cuda()\n",
        "  target_network = QNetwork(state_size, action_size).cuda()\n",
        "  target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "  # Init replay buffer\n",
        "  memory = []\n",
        "\n",
        "  # Begin main loop\n",
        "  results_dqn = []\n",
        "  global_step = 0\n",
        "  loop = tqdm(total=epochs, position=0, leave=False)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # Reset environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    cum_reward = 0  # Track cumulative reward per episode\n",
        "\n",
        "    # Begin episode\n",
        "    while not done and cum_reward < 200:  # End after 200 steps \n",
        "      # Select e-greedy action\n",
        "      action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
        "\n",
        "      # Take step\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      # env.render()\n",
        "\n",
        "      # Store step in replay buffer\n",
        "      memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "      cum_reward += reward\n",
        "      global_step += 1  # Increment total steps\n",
        "      state = next_state  # Set current state\n",
        "\n",
        "      # If time to train\n",
        "      if global_step > start_training and global_step % learn_frequency == 0:\n",
        "\n",
        "        # Sample batch\n",
        "        batch = prepare_batch(memory, batch_size)\n",
        "        \n",
        "        # Train\n",
        "        learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
        "\n",
        "    # Print results at end of episode\n",
        "    results_dqn.append(cum_reward)\n",
        "    loop.update(1)\n",
        "    loop.set_description('Episodes: {} Reward: {}'.format(epoch, cum_reward))\n",
        "  \n",
        "  return results_dqn\n",
        "\n",
        "results_dqn = dqn_main()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWYwytCDC3aw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1ab05f96-153b-4128-85b5-445058a0708e"
      },
      "source": [
        "plt.plot(results_dqn)\n",
        "plt.show()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN9yy5EWVNz0"
      },
      "source": [
        "## Part 2: PPO\n",
        "\n",
        "Proximal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a type of policy gradient method. Instead of calculating Q-values, we train a network $\\pi$ to optimize the probability of taking good actions directly, using states as inputs and actions as outputs. PPO also uses a value network $V$ that estimates state values in order to estimate the advantage $\\hat{A}$. \n",
        "\n",
        "Tuples of state, action distribution, action taken, and return $(s,\\pi(s), a,\\hat{R})$ are gathered for several rollouts. After training on this experience, these tuples are discarded and new experience is gathered.\n",
        "\n",
        "Loss for the value network and the policy network are calculated according to the following formula:\n",
        "\n",
        "$Loss=ValueLoss+PolicyLoss$\n",
        "\n",
        "$ValueLoss=\\frac{1}{N}\\sum \\bigg(\\hat{R} - V(s) \\bigg)^2 $\n",
        "\n",
        "$PolicyLoss=-\\frac{1}{N}\\sum \\min\\bigg( \\frac{\\pi'(a|s)}{\\pi(a|s)} \\hat{A}, clip(\\frac{\\pi'(a|s)}{\\pi(a|s)},1-\\epsilon,1+\\epsilon) \\hat{A} \\bigg) $\n",
        "\n",
        "$\\hat{R}_t = \\sum_{i=t}^H \\gamma^{i-1}r_i$\n",
        "\n",
        "$\\hat{A}_t=\\hat{R}_t-V(s_t)$\n",
        "\n",
        "Here, $\\pi'(a|s)$ is the probability of taking an action given a state under the current policy and $\\pi(a|s)$ is the probability of taking an action given a state under the policy used to gather data. In the loss function, $a$ is the action your agent actually took and is sampled from memory. \n",
        "\n",
        "Additionally, the $clip$ function clips the value of the first argument according to the lower and upper bounds in the second and third arguments resectively.\n",
        "\n",
        "Another important note: Your the calculation of your advantage $\\hat{A}$ should not permit gradient flow from your policy loss calculation. In other words, make sure to call `.detach()` on your advantage.\n",
        "\n",
        "### TODO\n",
        "\n",
        "- Implement calculate return method\n",
        "- Implement get action method\n",
        "- Implement PPO learning algorithm\n",
        "- Train PPO on cartpole\n",
        "- Display learning curves with average episodic reward per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsm1pILHVcEp"
      },
      "source": [
        "def calculate_return(memory, rollout, gamma):\n",
        "  \"\"\"Return memory with calculated return in experience tuple\n",
        "\n",
        "    Args:\n",
        "        memory (list): (state, action, action_dist, return) tuples\n",
        "        rollout (list): (state, action, action_dist, reward) tuples from last rollout\n",
        "        gamma (float): discount factor\n",
        "\n",
        "    Returns:\n",
        "        list: memory updated with (state, action, action_dist, return) tuples from rollout\n",
        "  \"\"\"\n",
        "\n",
        "    # easy to calculate backwards starting at time t\n",
        "    # memory is initially empty, we need add to it, and return it.\n",
        "\n",
        "  reverse = rollout[::-1]\n",
        "  temp = []\n",
        "\n",
        "  for i in range(len(reverse)):\n",
        "    state, action ,action_dist, reward = reverse[i]\n",
        "    if i == 0:\n",
        "      # reward = reward.cuda()\n",
        "      temp.append((state, action, action_dist, reward))\n",
        "\n",
        "    else:\n",
        "      R = (reward + gamma * reverse[i-1][3])\n",
        "      temp.append((state, action, action_dist, R))\n",
        "  memory = temp[::-1]\n",
        "  \n",
        "\n",
        "  return memory\n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def get_action_ppo(network, state):\n",
        "  \"\"\"Sample action from the distribution obtained from the policy network\n",
        "\n",
        "    Args:\n",
        "        network (PolicyNetwork): Policy Network\n",
        "        state (np-array): current state, size (state_size)\n",
        "\n",
        "    Returns:\n",
        "        int: action sampled from output distribution of policy network\n",
        "        array: output distribution of policy network\n",
        "  \"\"\"\n",
        "  state = torch.from_numpy(state).float().unsqueeze(0).cuda()\n",
        "\n",
        "  action_dist = network(state).cuda()\n",
        "\n",
        "  action = torch.multinomial(action_dist, 1) # torch.multinomial()\n",
        "\n",
        "  return action.item(), action_dist\n",
        "  \n",
        "\n",
        "def learn_ppo(optim, policy, value, memory_dataloader, epsilon, policy_epochs):\n",
        "  \"\"\"Implement PPO policy and value network updates. Iterate over your entire \n",
        "     memory the number of times indicated by policy_epochs.    \n",
        "\n",
        "    Args:\n",
        "        optim (Adam): value and policy optimizer\n",
        "        policy (PolicyNetwork): Policy Network\n",
        "        value (ValueNetwork): Value Network\n",
        "        memory_dataloader (DataLoader): dataloader with (state, action, action_dist, return, discounted_sum_rew) tensors\n",
        "        epsilon (float): trust region\n",
        "        policy_epochs (int): number of times to iterate over all memory\n",
        "  \"\"\"\n",
        "  for epoch in range(policy_epochs):\n",
        "    for batch in memory_dataloader:\n",
        "      optim.zero_grad()\n",
        "\n",
        "      # R = ????\n",
        "\n",
        "      state, action, action_dist, R = batch\n",
        "\n",
        "\n",
        "      print(\"State: \", state.size())\n",
        "      print(\"action: \", action.size())\n",
        "      print(\"action_dist: \", action_dist.size())\n",
        "      print(\"R: \", R.size())\n",
        "\n",
        "\n",
        "      state = state.float().cuda()\n",
        "      #action = action\n",
        "      action_dist = action_dist.float()\n",
        "      R = R.float().cuda()\n",
        "\n",
        "      value_loss = nn.functional.mse_loss(value(state),R)\n",
        "      A = None\n",
        "      with torch.no_grad():\n",
        "        A = R - value(state)\n",
        "      # A = A.detach()\n",
        "\n",
        "      print(\"value loss: \", value_loss.size())\n",
        "      print(\"A: \", A.size())\n",
        "\n",
        "      a_one_hot_encoding = nn.functional.one_hot(action.long(), num_classes=2).bool()\n",
        "      numerator = policy(state)\n",
        "      denominator = action_dist.squeeze(1)\n",
        "\n",
        "      print(\"a_one_hot: \", a_one_hot_encoding.size())\n",
        "      print(\"numerator: \", numerator.size())\n",
        "      print(\"denominator: \", denominator.size())\n",
        "\n",
        "      policy_ratio = (numerator[a_one_hot_encoding]/denominator[a_one_hot_encoding]).unsqueeze(1)\n",
        "      # print(\"\\n Policy size: \", policy_ratio.size())\n",
        "      clip_policy_ratio = torch.clamp(policy_ratio, 1-epsilon, 1+epsilon) #epsilon is .2 in this case\n",
        "  \n",
        "      #print(\"BIG TENSOR SIZE: \", cat_ratio.size())\n",
        "\n",
        "      policy_loss = -1*torch.min(torch.cat((policy_ratio, clip_policy_ratio), dim=1),dim=0)[0].mean()\n",
        "      print(\"policy loss: \", policy_loss)\n",
        "      \n",
        "      torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "      loss = value_loss + policy_loss\n",
        "      loss.backward(retain_graph=True)\n",
        "      optim.step()\n",
        "  return"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6RXma_-vSGX"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8URnP8xvTTG"
      },
      "source": [
        "# Dataset that wraps memory for a dataloader\n",
        "class RLDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    super().__init__()\n",
        "    self.data = []\n",
        "    for d in data:\n",
        "      self.data.append(d)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index]\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "# Policy Network\n",
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "    \n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, action_size),\n",
        "                             nn.Softmax(dim=1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"Get policy from state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          action_dist (tensor): probability distribution over actions (batch x action_size)\n",
        "    \"\"\"\n",
        "    return self.net(x)\n",
        "  \n",
        "\n",
        "# Value Network\n",
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self, state_size):\n",
        "    super().__init__()\n",
        "    hidden_size = 8\n",
        "  \n",
        "    self.net = nn.Sequential(nn.Linear(state_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, hidden_size),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(hidden_size, 1))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"Estimate value given state\n",
        "\n",
        "      Args:\n",
        "          state (tensor): current state, size (batch x state_size)\n",
        "\n",
        "      Returns:\n",
        "          value (tensor): estimated value, size (batch)\n",
        "    \"\"\"\n",
        "    return self.net(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBD_R_e01Qb"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX_Bv4M4MyY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "39ff6677-9863-480f-dfb2-210ab181dcfb"
      },
      "source": [
        "def ppo_main():\n",
        "  # Hyper parameters\n",
        "  lr = 1e-3\n",
        "  epochs = 20\n",
        "  env_samples = 100\n",
        "  gamma = 0.9\n",
        "  batch_size = 256\n",
        "  epsilon = 0.2\n",
        "  policy_epochs = 5\n",
        "\n",
        "  # Init environment \n",
        "  state_size = 4\n",
        "  action_size = 2\n",
        "  env = gym.make('CartPole-v1')\n",
        "\n",
        "  # Init networks\n",
        "  policy_network = PolicyNetwork(state_size, action_size).cuda()\n",
        "  value_network = ValueNetwork(state_size).cuda()\n",
        "\n",
        "  # Init optimizer\n",
        "  optim = torch.optim.Adam(chain(policy_network.parameters(), value_network.parameters()), lr=lr)\n",
        "\n",
        "  # Start main loop\n",
        "  results_ppo = []\n",
        "  loop = tqdm(total=epochs, position=0, leave=False)\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    memory = []  # Reset memory every epoch\n",
        "    rewards = []  # Calculate average episodic reward per epoch\n",
        "\n",
        "    # Begin experience loop\n",
        "    for episode in range(env_samples):\n",
        "      \n",
        "      # Reset environment\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      rollout = []\n",
        "      cum_reward = 0  # Track cumulative reward\n",
        "\n",
        "      # Begin episode\n",
        "      while not done and cum_reward < 200:  # End after 200 steps   \n",
        "        # Get action\n",
        "        action, action_dist = get_action_ppo(policy_network, state)\n",
        "        \n",
        "        # Take step\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # env.render()\n",
        "\n",
        "        # Store step\n",
        "        rollout.append((state, action, action_dist, reward))\n",
        "\n",
        "        cum_reward += reward\n",
        "        state = next_state  # Set current state\n",
        "\n",
        "      # Calculate returns and add episode to memory\n",
        "      memory = calculate_return(memory, rollout, gamma)\n",
        "\n",
        "      rewards.append(cum_reward)\n",
        "      \n",
        "    # Train\n",
        "    dataset = RLDataset(memory)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    learn_ppo(optim, policy_network, value_network, loader, epsilon, policy_epochs)\n",
        "    \n",
        "    # Print results\n",
        "    results_ppo.extend(rewards)  # Store rewards for this epoch\n",
        "    loop.update(1)\n",
        "    loop.set_description(\"Epochs: {} Reward: {}\".format(epoch, results_ppo[-1]))\n",
        "\n",
        "  return results_ppo\n",
        "\n",
        "results_ppo = ppo_main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:  torch.Size([14, 4])\n",
            "action:  torch.Size([14])\n",
            "action_dist:  torch.Size([14, 1, 2])\n",
            "R:  torch.Size([14])\n",
            "value loss:  torch.Size([])\n",
            "A:  torch.Size([14, 14])\n",
            "a_one_hot:  torch.Size([14, 2])\n",
            "numerator:  torch.Size([14, 2])\n",
            "denominator:  torch.Size([14, 2])\n",
            "policy loss:  tensor(-1.0000, device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:94: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:  torch.Size([14, 4])\n",
            "action:  torch.Size([14])\n",
            "action_dist:  torch.Size([14, 1, 2])\n",
            "R:  torch.Size([14])\n",
            "value loss:  torch.Size([])\n",
            "A:  torch.Size([14, 14])\n",
            "a_one_hot:  torch.Size([14, 2])\n",
            "numerator:  torch.Size([14, 2])\n",
            "denominator:  torch.Size([14, 2])\n",
            "policy loss:  tensor(-0.9989, device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b579181b65ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresults_ppo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mresults_ppo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b579181b65ef>\u001b[0m in \u001b[0;36mppo_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRLDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mlearn_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-caf0607084dc>\u001b[0m in \u001b[0;36mlearn_ppo\u001b[0;34m(optim, policy, value, memory_dataloader, epsilon, policy_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [8, 2]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLXetCMpC1DE"
      },
      "source": [
        "plt.plot(results_ppo)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH4LLyWvsG8C"
      },
      "source": [
        "pNet = PolicyNetwork(4, 2)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMIt9IFksPhF"
      },
      "source": [
        "state = torch.rand(256, 1, 4)\n",
        "action = "
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok1Yf_tfysz1"
      },
      "source": [
        "A = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "3TBavBhfsYDu",
        "outputId": "48dd2168-154d-4868-dd9c-f8807dd466c6"
      },
      "source": [
        "policy_loss = -1*torch.min((policy_ratio * A, clip_policy_ratio * A), dim = 1).mean()\n"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-261-7e26a5960a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pEOYs49yefZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}